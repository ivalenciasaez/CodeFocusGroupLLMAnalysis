{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### From transcript via chunking to summary. I incorrectly used different prompts to retain as much information, I didnt think it would work out and it cost a lot of time. If I were to do this over, I would standardize my prompts across all analysis and summarization functions ###\n",
    "\n",
    "%pip show openai\n",
    "\n",
    "# Install the OpenAI package\n",
    "%pip install openai\n",
    "\n",
    "# Setup OpenAI Client\n",
    "client = OpenAI(\n",
    "  api_key=os.environ.get('OPENAI_API_KEY', '')\n",
    ")\n",
    "\n",
    "\n",
    "import openai\n",
    "import os\n",
    "\n",
    "# Import the new OpenAI client class\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup OpenAI Client\n",
    "client = OpenAI(\n",
    "  api_key=os.environ.get('OPENAI_API_KEY', '')\n",
    ")\n",
    "\n",
    "# Simple test to verify the setup\n",
    "def test_openai_api():\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"Say this is a test\"}\n",
    "            ],\n",
    "            temperature=0  # Set temperature to 0 for deterministic results\n",
    "        )\n",
    "        print(response.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_openai_api()\n",
    "\n",
    "# Function to read the transcript with different encoding\n",
    "def read_transcript(file_path):\n",
    "    with open(file_path, 'r', encoding='iso-8859-1') as file:\n",
    "        transcript = file.read()\n",
    "    return transcript\n",
    "\n",
    "# Use the full path to the transcript file\n",
    "file_path = '/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/Focusgroep Voedselbank Transcriptie.txt'\n",
    "transcript = read_transcript(file_path)\n",
    "\n",
    "# Print the first 500 characters to verify the content\n",
    "print(transcript[:500])\n",
    "\n",
    "print(\"Transcript read successfully.\")\n",
    "\n",
    "# Function to split transcript based on word ranges\n",
    "def split_by_word_ranges(text, word_ranges):\n",
    "    words = text.split()\n",
    "    chunks = [' '.join(words[start:end]) for start, end in word_ranges]\n",
    "    return chunks\n",
    "\n",
    "# Define word ranges based on exercises\n",
    "exercise_word_ranges = [\n",
    "    (0, 1644),\n",
    "    (1644, 3691),\n",
    "    (3691, 7153),  # corrected the range to avoid overlap\n",
    "    (7153, 10676),\n",
    "    (10676, 13249),\n",
    "    (13249, 18546)\n",
    "]\n",
    "\n",
    "# Split the transcript into chunks based on word ranges\n",
    "transcript_chunks = split_by_word_ranges(transcript, exercise_word_ranges)\n",
    "\n",
    "# Print the first chunk to verify\n",
    "print(transcript_chunks[0])\n",
    "print(\"Transcript split into exercise chunks successfully.\")\n",
    "\n",
    "# Write each chunk to a separate file\n",
    "output_dir = '/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "for i, chunk in enumerate(transcript_chunks):\n",
    "    with open(os.path.join(output_dir, f'chunk_{i+1}.txt'), 'w', encoding='utf-8') as file:\n",
    "        file.write(chunk)\n",
    "\n",
    "print(f\"Chunks written to files in {output_dir} successfully.\")\n",
    "\n",
    "# Function to split text into smaller sub-chunks if it exceeds the token limit\n",
    "def split_into_subchunks(text, max_tokens=8192):\n",
    "    words = text.split()\n",
    "    subchunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for the space\n",
    "        if current_length > max_tokens:\n",
    "            subchunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        subchunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return subchunks\n",
    "\n",
    "# Function to summarize each chunk\n",
    "def summarize_chunk(chunk):\n",
    "    subchunks = split_into_subchunks(chunk, max_tokens=1500)  # use a smaller token limit for safety\n",
    "    summaries = []\n",
    "    for subchunk in subchunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Please summarize the following text:\\n\\n{subchunk}\"}\n",
    "                ],\n",
    "                temperature=0  # Set temperature to 0 for deterministic results\n",
    "            )\n",
    "            summary = response.choices[0].message.content.strip()\n",
    "            summaries.append(summary)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while summarizing: {e}\")\n",
    "            return None\n",
    "    return ' '.join(summaries)\n",
    "\n",
    "# Summarize each chunk and save the summaries\n",
    "summaries = []\n",
    "for i, chunk in enumerate(transcript_chunks):\n",
    "    summary = summarize_chunk(chunk)\n",
    "    if summary:\n",
    "        summaries.append(summary)\n",
    "        print(f\"Summary for chunk {i+1}:\\n{summary}\\n\")\n",
    "\n",
    "# Write summaries to a file\n",
    "summaries_file_path = os.path.join(output_dir, 'summaries.txt')\n",
    "with open(summaries_file_path, 'w', encoding='utf-8') as file:\n",
    "    for i, summary in enumerate(summaries):\n",
    "        file.write(f\"Summary for chunk {i+1}:\\n{summary}\\n\\n\")\n",
    "\n",
    "print(f\"Summaries written to {summaries_file_path} successfully.\")\n",
    "\n",
    "##\n",
    "\n",
    "# Function to analyze well-being topics in each chunk\n",
    "def analyze_chunk(chunk):\n",
    "    subchunks = split_into_subchunks(chunk, max_tokens=1500)  # use a smaller token limit for safety\n",
    "    topics_responses = []\n",
    "    interrelations_responses = []\n",
    "    for subchunk in subchunks:\n",
    "        try:\n",
    "            # Identify well-being topics\n",
    "            topics_response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Identify and list the topics discussed in the following text that participants believe impact their well-being, either positively or negatively. Provide quotes to support each topic identified. Consider any aspect of their lives that they mention as influencing their quality of life, well-being, or overall life satisfaction:\\n\\n{subchunk}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            topics_responses.append(topics_response.choices[0].message.content.strip())\n",
    "            \n",
    "            # Explain interrelations of well-being topics\n",
    "            interrelations_response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Explain how the identified topics related to well-being are interrelated in the following text. Provide quotes or specific examples from the text to support your explanation. Focus on how these topics impact participants' well-being and their experiences with the informal foodbank:\\n\\n{subchunk}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            interrelations_responses.append(interrelations_response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while analyzing: {e}\")\n",
    "            return None, None\n",
    "    \n",
    "    return ' '.join(topics_responses), ' '.join(interrelations_responses)\n",
    "\n",
    "# Analyze each chunk and save the results\n",
    "analysis_results = []\n",
    "for i, chunk in enumerate(transcript_chunks):\n",
    "    topics, interrelations = analyze_chunk(chunk)\n",
    "    if topics and interrelations:\n",
    "        analysis_results.append({\n",
    "            \"chunk\": i + 1,\n",
    "            \"topics\": topics,\n",
    "            \"interrelations\": interrelations\n",
    "        })\n",
    "        print(f\"Analysis for chunk {i+1}:\\nTopics:\\n{topics}\\nInterrelations:\\n{interrelations}\\n\")\n",
    "\n",
    "# Write the partial analyses to separate files\n",
    "for result in analysis_results:\n",
    "    partial_analysis_file_path = os.path.join(output_dir, f'partial_analysis_chunk_{result[\"chunk\"]}.txt')\n",
    "    with open(partial_analysis_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(f\"Analysis for chunk {result['chunk']}:\\nTopics:\\n{result['topics']}\\nInterrelations:\\n{result['interrelations']}\\n\\n\")\n",
    "\n",
    "print(\"Partial analyses written to separate files successfully.\")\n",
    "\n",
    "# Combine all partial analyses into one document\n",
    "combined_analysis_file_path = os.path.join(output_dir, 'combined_analysis.txt')\n",
    "with open(combined_analysis_file_path, 'w', encoding='utf-8') as combined_file:\n",
    "    for result in analysis_results:\n",
    "        combined_file.write(f\"Analysis for chunk {result['chunk']}:\\nTopics:\\n{result['topics']}\\nInterrelations:\\n{result['interrelations']}\\n\\n\")\n",
    "\n",
    "print(f\"Combined analysis written to {combined_analysis_file_path} successfully.\")\n",
    "\n",
    "\n",
    "###\n",
    "\n",
    "\n",
    "# Function to combine and analyze all partial analyses into one encapsulating analysis\n",
    "def combine_and_analyze_analyses(partial_analyses):\n",
    "    combined_analysis_text = \"\\n\\n\".join(partial_analyses)\n",
    "    combined_analysis_chunks = split_into_subchunks(combined_analysis_text, max_tokens=5000)  # Split into smaller chunks\n",
    "    combined_analysis_responses = []\n",
    "\n",
    "    for chunk in combined_analysis_chunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Analyze the following combined analyses and provide a comprehensive analysis that encapsulates the key insights, themes, and interrelations identified in each partial analysis:\\n\\n{chunk}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            combined_analysis_responses.append(response.choices[0].message.content.strip())\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while combining and analyzing the analyses: {e}\")\n",
    "            return None\n",
    "\n",
    "    combined_analysis = ' '.join(combined_analysis_responses)\n",
    "    return combined_analysis\n",
    "\n",
    "# Function to split text into smaller sub-chunks if it exceeds the token limit\n",
    "def split_into_subchunks(text, max_tokens=8192):\n",
    "    words = text.split()\n",
    "    subchunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "    \n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for the space\n",
    "        if current_length > max_tokens:\n",
    "            subchunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "    \n",
    "    if current_chunk:\n",
    "        subchunks.append(' '.join(current_chunk))\n",
    "    \n",
    "    return subchunks\n",
    "\n",
    "# Read the partial analyses from the files\n",
    "partial_analyses = []\n",
    "for i in range(1, 7):\n",
    "    partial_analysis_file_path = os.path.join(output_dir, f'partial_analysis_chunk_{i}.txt')\n",
    "    with open(partial_analysis_file_path, 'r', encoding='utf-8') as file:\n",
    "        partial_analyses.append(file.read())\n",
    "\n",
    "# Combine and analyze the partial analyses\n",
    "combined_analysis = combine_and_analyze_analyses(partial_analyses)\n",
    "\n",
    "# Write the combined analysis to a file\n",
    "if combined_analysis:\n",
    "    combined_analysis_file_path = os.path.join(output_dir, 'combined_analysis_encapsulated.txt')\n",
    "    with open(combined_analysis_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(combined_analysis)\n",
    "\n",
    "    print(f\"Combined analysis written to {combined_analysis_file_path} successfully.\")\n",
    "else:\n",
    "    print(\"Failed to produce a combined analysis.\")\n",
    "\n",
    "##\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup OpenAI Client\n",
    "client = OpenAI(\n",
    "    api_key=''\n",
    ")\n",
    "\n",
    "# Test the setup with a simple call\n",
    "def test_openai_api():\n",
    "    try:\n",
    "        chat_completion = client.chat.completions.create(\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": \"Say this is a test\",\n",
    "                }\n",
    "            ],\n",
    "            model=\"gpt-4\",\n",
    "        )\n",
    "        print(chat_completion.choices[0].message.content.strip())\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Run the test\n",
    "test_openai_api()\n",
    "\n",
    "# Read and split the combined analysis\n",
    "combined_analysis_file_path = '/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/combined_analysis.txt'\n",
    "with open(combined_analysis_file_path, 'r', encoding='utf-8') as file:\n",
    "    combined_analysis_text = file.read()\n",
    "\n",
    "# Function to split text into smaller sub-chunks if it exceeds the token limit\n",
    "def split_into_subchunks(text, max_tokens=2000):\n",
    "    words = text.split()\n",
    "    subchunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for the space\n",
    "        if current_length > max_tokens:\n",
    "            subchunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        subchunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return subchunks\n",
    "\n",
    "# Function to summarize the combined analysis\n",
    "def summarize_combined_analysis(combined_text):\n",
    "    subchunks = split_into_subchunks(combined_text, max_tokens=2000)\n",
    "    condensed_parts = []\n",
    "    for subchunk in subchunks:\n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=\"gpt-4\",\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                    {\"role\": \"user\", \"content\": f\"Condense the following text into a summary that fits within one word page:\\n\\n{subchunk}\"}\n",
    "                ],\n",
    "                temperature=0\n",
    "            )\n",
    "            condensed_part = response.choices[0].message.content.strip()\n",
    "            condensed_parts.append(condensed_part)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred while condensing the summary: {e}\")\n",
    "            return None\n",
    "    final_combined_summary = ' '.join(condensed_parts)\n",
    "    return final_combined_summary\n",
    "\n",
    "# Condense the combined analysis\n",
    "condensed_summary = summarize_combined_analysis(combined_analysis_text)\n",
    "if condensed_summary:\n",
    "    condensed_summary_file_path = os.path.join(output_dir, 'condensed_combined_analysis.txt')\n",
    "    with open(condensed_summary_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(condensed_summary)\n",
    "    print(f\"Condensed combined analysis written to {condensed_summary_file_path} successfully.\")\n",
    "else:\n",
    "    print(\"Failed to produce condensed summary.\")\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "# Setup OpenAI Client\n",
    "client = OpenAI(\n",
    "    api_key=os.environ.get('OPENAI_API_KEY', '')\n",
    ")\n",
    "\n",
    "# Path to the condensed combined analysis file\n",
    "condensed_summary_file_path = ''\n",
    "/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/condensed_combined_analysis.txt\n",
    "# Function to read the condensed combined analysis file\n",
    "def read_condensed_analysis(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        condensed_analysis_text = file.read()\n",
    "    return condensed_analysis_text\n",
    "\n",
    "# Read the condensed analysis text\n",
    "condensed_analysis_text = read_condensed_analysis(condensed_summary_file_path)\n",
    "\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "import os\n",
    "\n",
    "# Function to read the condensed combined analysis file\n",
    "def read_condensed_analysis(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        condensed_analysis_text = file.read()\n",
    "    return condensed_analysis_text\n",
    "\n",
    "# Path to the condensed combined analysis file\n",
    "condensed_summary_file_path = '/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/condensed_combined_analysis.txt'\n",
    "\n",
    "# Read the condensed analysis text\n",
    "condensed_analysis_text = read_condensed_analysis(condensed_summary_file_path)\n",
    "\n",
    "# Print the first 1000 characters to verify\n",
    "print(condensed_analysis_text[:1000])\n",
    "\n",
    "\n",
    "##\n",
    "\n",
    "#### Enter: Prompt 1 #### (Bad analysis I know, should have used the same prompt from the beginning but I did not manage, too many errors, too unfamiliar with python, I was too messy) ###\n",
    "\n",
    "from openai import OpenAI\n",
    "import tiktoken\n",
    "\n",
    "# Setup OpenAI Client\n",
    "client = OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY', '')\n",
    ")\n",
    "\n",
    "# Function to count tokens using tiktoken\n",
    "def count_tokens(text, model=\"gpt-4o\"):\n",
    "    enc = tiktoken.encoding_for_model(model)\n",
    "    tokens = enc.encode(text)\n",
    "    return len(tokens)\n",
    "\n",
    "# Function to split text into smaller sub-chunks if it exceeds the token limit\n",
    "def split_into_subchunks(text, max_tokens=1000):\n",
    "    words = text.split()\n",
    "    subchunks = []\n",
    "    current_chunk = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        current_length += len(word) + 1  # +1 for the space\n",
    "        if current_length > max_tokens:\n",
    "            subchunks.append(' '.join(current_chunk))\n",
    "            current_chunk = [word]\n",
    "            current_length = len(word) + 1\n",
    "        else:\n",
    "            current_chunk.append(word)\n",
    "\n",
    "    if current_chunk:\n",
    "        subchunks.append(' '.join(current_chunk))\n",
    "\n",
    "    return subchunks\n",
    "\n",
    "# Split the condensed analysis text into smaller chunks\n",
    "subchunks = split_into_subchunks(condensed_analysis_text, max_tokens=1000)\n",
    "print(f\"Number of subchunks created: {len(subchunks)}\")\n",
    "\n",
    "# Function to summarize the text with the matrix development prompt\n",
    "def summarize_text(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Please summarize the following text with a focus on identifying indicator pairs related to well-being. Consider factors such as frequency of co-occurrence, emotional context, and moments of agreement or disagreement:\\n\\n{text}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        summary = response.choices[0].message.content.strip()\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while summarizing the text: {e}\")\n",
    "        return None\n",
    "\n",
    "# Summarize each subchunk\n",
    "summaries = []\n",
    "for chunk in subchunks:\n",
    "    summary = summarize_text(chunk)\n",
    "    if summary:\n",
    "        summaries.append(summary)\n",
    "    else:\n",
    "        break  # Stop if there's an error to avoid unnecessary requests\n",
    "\n",
    "# Combine all summaries into one\n",
    "combined_summary = ' '.join(summaries)\n",
    "\n",
    "# Save the combined summary to the specified file\n",
    "output_file_path = \"/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/combined_summary.txt\"\n",
    "with open(output_file_path, 'w') as file:\n",
    "    file.write(combined_summary)\n",
    "\n",
    "# Print the combined summary to verify\n",
    "combined_summary_length = count_tokens(combined_summary)\n",
    "print(f\"Length of combined summary: {combined_summary_length} tokens\")\n",
    "print(f\"Combined Summary:\\n{combined_summary[:1000]}\")  # Print the first 1000 characters of the summary\n",
    "\n",
    "\n",
    "##### Enter: Prompt 2 Matrix development ####\n",
    "\n",
    "# Read the combined summary from the specified file\n",
    "combined_summary_path = \"/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/combined_summary.txt\"\n",
    "with open(combined_summary_path, 'r') as file:\n",
    "    combined_summary = file.read()\n",
    "\n",
    "# Function to generate the symmetric matrix using GPT-4-turbo\n",
    "def generate_symmetric_matrix(text):\n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-4-turbo\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Create a symmetric matrix of indicator pairs related to well-being from the following text. Include all unique indicators as both rows and columns. For each cell, provide the tie strength from 0 to 5, considering factors such as frequency of co-occurrence, emotional context, and moments of agreement or disagreement. Assign a tie strength of 0 for the weakest links where the indicators have minimal or no connection, and replace all 1's with 0's. Justify the tie strength for each pair.\\n\\n{text}\"}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        response_message = response.choices[0].message.content.strip()\n",
    "        return response_message\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while generating the symmetric matrix: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate the symmetric matrix\n",
    "symmetric_matrix_text = generate_symmetric_matrix(combined_summary)\n",
    "\n",
    "# Print the generated symmetric matrix text to verify\n",
    "print(symmetric_matrix_text)\n",
    "\n",
    "# Save the symmetric matrix to a txt file\n",
    "output_txt_file_path = \"/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/symmetric_matrix.txt\"\n",
    "with open(output_txt_file_path, 'w') as file:\n",
    "    file.write(symmetric_matrix_text)\n",
    "\n",
    "# Convert the symmetric matrix text to a CSV format and save it\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Assume the symmetric matrix is formatted as a markdown table in the response text\n",
    "matrix_df = pd.read_csv(StringIO(symmetric_matrix_text), sep=\"|\", skipinitialspace=True)\n",
    "\n",
    "# Clean up the column headers (strip any leading/trailing spaces)\n",
    "matrix_df.columns = matrix_df.columns.str.strip()\n",
    "\n",
    "# Save the dataframe to a CSV file\n",
    "output_csv_file_path = \"/Users/eliecervalencia/Documents/Transcriptie Voedselbank Focusgroep/chunks/symmetric_matrix.csv\"\n",
    "matrix_df.to_csv(output_csv_file_path, index=False)\n",
    "\n",
    "\n",
    "#### Network visualization ####\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DataFrame from the provided matrix\n",
    "data = {\n",
    "    'Indicator': ['Financial Struggles', 'Food Bank Assistance', 'Unemployment', 'Cost of Living', \n",
    "                  'Community Support', 'Dependence on Food Bank Services', 'Social Support', 'Access to Food', \n",
    "                  'Mental Health', 'Physical Health', 'Energy Consumption', 'Reluctance to Use Gas', \n",
    "                  'National Issues', 'Perceived Neglect of Local Problems', 'Housing', 'Future Prospects'],\n",
    "    'Financial Struggles': [0, 5, 4, 5, 2, 5, 3, 4, 4, 3, 2, 2, 3, 4, 4, 3],\n",
    "    'Food Bank Assistance': [5, 0, 3, 4, 5, 5, 5, 5, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Unemployment': [4, 3, 0, 5, 2, 4, 2, 3, 3, 2, 1, 1, 2, 3, 3, 4],\n",
    "    'Cost of Living': [5, 4, 5, 0, 2, 4, 2, 4, 3, 2, 4, 4, 3, 4, 4, 3],\n",
    "    'Community Support': [2, 5, 2, 2, 0, 5, 5, 4, 2, 2, 1, 1, 2, 3, 2, 2],\n",
    "    'Dependence on Food Bank Services': [5, 5, 4, 4, 5, 0, 5, 5, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Social Support': [3, 5, 2, 2, 5, 5, 0, 5, 4, 3, 1, 1, 2, 3, 2, 2],\n",
    "    'Access to Food': [4, 5, 3, 4, 4, 5, 5, 0, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Mental Health': [4, 3, 3, 3, 2, 3, 4, 3, 0, 5, 2, 2, 2, 3, 3, 3],\n",
    "    'Physical Health': [3, 3, 2, 2, 2, 3, 3, 3, 5, 0, 2, 2, 2, 3, 3, 2],\n",
    "    'Energy Consumption': [2, 2, 1, 4, 1, 2, 1, 2, 2, 2, 0, 5, 2, 3, 2, 1],\n",
    "    'Reluctance to Use Gas': [2, 2, 1, 4, 1, 2, 1, 2, 2, 2, 5, 0, 2, 3, 2, 1],\n",
    "    'National Issues': [3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 5, 3, 2],\n",
    "    'Perceived Neglect of Local Problems': [4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 5, 0, 4, 3],\n",
    "    'Housing': [4, 3, 3, 4, 2, 3, 2, 3, 3, 3, 2, 2, 3, 4, 0, 4],\n",
    "    'Future Prospects': [3, 2, 4, 3, 2, 2, 2, 2, 3, 2, 1, 1, 2, 3, 4, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).set_index('Indicator')\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for indicator in df.index:\n",
    "    G.add_node(indicator)\n",
    "\n",
    "# Add edges\n",
    "for i, indicator in enumerate(df.index):\n",
    "    for j, value in enumerate(df.iloc[i]):\n",
    "        if value > 0:\n",
    "            G.add_edge(indicator, df.columns[j], weight=value)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, seed=42)  # positions for all nodes\n",
    "\n",
    "# Draw nodes and labels\n",
    "nx.draw_networkx_nodes(G, pos, node_size=7000, node_color='skyblue', alpha=0.7)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "# Draw edges\n",
    "edges = nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]}' for u, v, d in G.edges(data=True)}, font_color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Well-being Indicator Network')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "%pip install networkx\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install scipy\n",
    "\n",
    "#\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the DataFrame from the provided matrix\n",
    "data = {\n",
    "    'Indicator': ['Financial Struggles', 'Food Bank Assistance', 'Unemployment', 'Cost of Living', \n",
    "                  'Community Support', 'Dependence on Food Bank Services', 'Social Support', 'Access to Food', \n",
    "                  'Mental Health', 'Physical Health', 'Energy Consumption', 'Reluctance to Use Gas', \n",
    "                  'National Issues', 'Perceived Neglect of Local Problems', 'Housing', 'Future Prospects'],\n",
    "    'Financial Struggles': [0, 5, 4, 5, 2, 5, 3, 4, 4, 3, 2, 2, 3, 4, 4, 3],\n",
    "    'Food Bank Assistance': [5, 0, 3, 4, 5, 5, 5, 5, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Unemployment': [4, 3, 0, 5, 2, 4, 2, 3, 3, 2, 1, 1, 2, 3, 3, 4],\n",
    "    'Cost of Living': [5, 4, 5, 0, 2, 4, 2, 4, 3, 2, 4, 4, 3, 4, 4, 3],\n",
    "    'Community Support': [2, 5, 2, 2, 0, 5, 5, 4, 2, 2, 1, 1, 2, 3, 2, 2],\n",
    "    'Dependence on Food Bank Services': [5, 5, 4, 4, 5, 0, 5, 5, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Social Support': [3, 5, 2, 2, 5, 5, 0, 5, 4, 3, 1, 1, 2, 3, 2, 2],\n",
    "    'Access to Food': [4, 5, 3, 4, 4, 5, 5, 0, 3, 3, 2, 2, 2, 3, 3, 2],\n",
    "    'Mental Health': [4, 3, 3, 3, 2, 3, 4, 3, 0, 5, 2, 2, 2, 3, 3, 3],\n",
    "    'Physical Health': [3, 3, 2, 2, 2, 3, 3, 3, 5, 0, 2, 2, 2, 3, 3, 2],\n",
    "    'Energy Consumption': [2, 2, 1, 4, 1, 2, 1, 2, 2, 2, 0, 5, 2, 3, 2, 1],\n",
    "    'Reluctance to Use Gas': [2, 2, 1, 4, 1, 2, 1, 2, 2, 2, 5, 0, 2, 3, 2, 1],\n",
    "    'National Issues': [3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 2, 2, 0, 5, 3, 2],\n",
    "    'Perceived Neglect of Local Problems': [4, 3, 3, 4, 3, 3, 3, 3, 3, 3, 3, 3, 5, 0, 4, 3],\n",
    "    'Housing': [4, 3, 3, 4, 2, 3, 2, 3, 3, 3, 2, 2, 3, 4, 0, 4],\n",
    "    'Future Prospects': [3, 2, 4, 3, 2, 2, 2, 2, 3, 2, 1, 1, 2, 3, 4, 0]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).set_index('Indicator')\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for indicator in df.index:\n",
    "    G.add_node(indicator)\n",
    "\n",
    "# Add edges\n",
    "for i, indicator in enumerate(df.index):\n",
    "    for j, value in enumerate(df.iloc[i]):\n",
    "        if value > 0:\n",
    "            G.add_edge(indicator, df.columns[j], weight=value)\n",
    "\n",
    "# Draw the graph using Kamada-Kawai layout\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.kamada_kawai_layout(G)  # Kamada-Kawai layout\n",
    "\n",
    "# Draw nodes and labels\n",
    "nx.draw_networkx_nodes(G, pos, node_size=7000, node_color='skyblue', alpha=0.7)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "# Draw edges\n",
    "edges = nx.draw_networkx_edges(G, pos, width=1.0, alpha=0.5)\n",
    "nx.draw_networkx_edge_labels(G, pos, edge_labels={(u, v): f'{d[\"weight\"]}' for u, v, d in G.edges(data=True)}, font_color='red')\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Well-being Indicator Network (Kamada-Kawai Layout)')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "\n",
    "# Assuming df is already created from the previous steps\n",
    "\n",
    "# Create the graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes\n",
    "for indicator in df.index:\n",
    "    G.add_node(indicator)\n",
    "\n",
    "# Add edges\n",
    "for i, indicator in enumerate(df.index):\n",
    "    for j, value in enumerate(df.iloc[i]):\n",
    "        if value > 0:\n",
    "            G.add_edge(indicator, df.columns[j], weight=value)\n",
    "\n",
    "# Calculate node sizes based on the degree (number of edges) with a scaling factor\n",
    "node_degrees = [G.degree(n) for n in G.nodes()]\n",
    "min_degree = min(node_degrees)\n",
    "max_degree = max(node_degrees)\n",
    "scaling_factor = 200  # Adjust as needed for visual clarity\n",
    "\n",
    "node_sizes = [scaling_factor * (G.degree(n) - min_degree + 1) for n in G.nodes()]\n",
    "\n",
    "# Calculate edge colors and transparency based on weight\n",
    "edge_weights = [d['weight'] for _, _, d in G.edges(data=True)]\n",
    "cmap = cm.get_cmap('Blues')\n",
    "norm = mcolors.Normalize(vmin=min(edge_weights), vmax=max(edge_weights))\n",
    "edge_colors = [cmap(norm(w)) for w in edge_weights]\n",
    "edge_alphas = [norm(w) for w in edge_weights]\n",
    "\n",
    "# Draw the graph using Kamada-Kawai layout\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.kamada_kawai_layout(G)  # Kamada-Kawai layout\n",
    "\n",
    "# Draw nodes and labels\n",
    "nx.draw_networkx_nodes(G, pos, node_size=node_sizes, node_color='skyblue', alpha=0.7)\n",
    "nx.draw_networkx_labels(G, pos, font_size=10)\n",
    "\n",
    "# Draw edges with varying thickness based on weight, colors, and transparency, and make them less curved\n",
    "ax = plt.gca()\n",
    "for (u, v, d), color, alpha in zip(G.edges(data=True), edge_colors, edge_alphas):\n",
    "    edge = FancyArrowPatch(\n",
    "        posA=pos[u], posB=pos[v],\n",
    "        connectionstyle='arc3,rad=0.1',  # Reduced curvature\n",
    "        arrowstyle='-',  # No arrow heads\n",
    "        color=color, alpha=alpha,\n",
    "        linewidth=d['weight']\n",
    "    )\n",
    "    ax.add_patch(edge)\n",
    "\n",
    "# Show the plot\n",
    "plt.title('Well-being Indicator Network (Kamada-Kawai Layout)')\n",
    "plt.show()\n",
    "\n",
    "#\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
